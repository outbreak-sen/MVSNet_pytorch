# æŠŠæ·±åº¦å›å½’é—®é¢˜ â†’ æ·±åº¦åˆ†ç±»é—®é¢˜
# å°†æ·±åº¦åˆ’åˆ†ä¸º K ä¸ª bin
# MVS å’Œ DA3 å„è‡ªå¯¹ bin æœ‰ soft prior
# èåˆç½‘ç»œè¾“å‡ºä¸€ä¸ª æ–°çš„ bin åˆ†å¸ƒ
# ç”¨ one-hot / soft-label loss
# ğŸ‘‰ è¿™æ˜¯ CVPR / ICCV æ·±åº¦ä¼°è®¡çš„ä¸»æµæ–¹å‘
# å› ä¸ºDA3çš„confä¸æ˜¯çœŸå®çš„cofï¼ŒDA3 / MVS çš„ conf åªå½±å“ logits

import torch
import torch.nn as nn
import torch.nn.functional as F

class DepthBinFusionNet(nn.Module):
    """
    Depth bin fusion with confidence-aware conditioning
    """
    def __init__(self, num_bins=64, hidden=64):
        super().__init__()
        self.num_bins = num_bins

        self.encoder = nn.Sequential(
            nn.Conv2d(4, hidden, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden, hidden, 3, padding=1),
            nn.ReLU(inplace=True),
        )

        self.bin_logits = nn.Conv2d(hidden, num_bins, 1)

    def forward(self, D_mvs, C_mvs, D_da, C_da):
        """
        conf is treated as raw feature
        """
        x = torch.cat([D_mvs, D_da, C_mvs, C_da], dim=1)
        feat = self.encoder(x)

        logits = self.bin_logits(feat)
        prob = F.softmax(logits, dim=1)

        # depth confidence from entropy
        conf = -torch.sum(prob * torch.log(prob + 1e-8), dim=1, keepdim=True)

        return prob, conf


def depth_bin_loss(prob, gt_depth, depth_bins):
    """
    prob: (B, K, H, W)
    gt_depth: (B, H, W)
    depth_bins: (B, K) or (1, K)
    """
    B, K, H, W = prob.shape
    # gt_depth: (B, H, W) -> (B, 1, H, W)
    # depth_bins: (B, K) -> (B, K, 1, 1)
    # è¿™æ ·å¹¿æ’­åå¾—åˆ°: (B, K, H, W)
    gt_depth_expanded = gt_depth.unsqueeze(1)  # (B, 1, H, W)
    depth_bins_expanded = depth_bins.view(B, K, 1, 1)  # (B, K, 1, 1)
    
    # hard assignment
    # è®¡ç®—æ¯ä¸ªåƒç´ ç‚¹ä¸æ¯ä¸ªæ·±åº¦åŒºé—´çš„çœŸå®è·ç¦»
    dist = torch.abs(gt_depth_expanded - depth_bins_expanded)  # (B, K, H, W)
    # è·ç¦»åˆ°æŸä¸ªbinè·ç¦»æœ€å°è¯´æ˜è¿™ä¸ªbinæ˜¯çœŸå®bin
    gt_bin = torch.argmin(dist, dim=1)  # (B, H, W)
    # é‚£ä¹ˆè¿™é‡Œå¯ä»¥è®¡ç®—ä¸€ä¸ªprobçš„çŒœæµ‹å¯¹ä¸å¯¹äº†ï¼Œå¯ä»¥æŠŠgtbingçœ‹ä½œæ ‡ç­¾ï¼Œè®¡ç®—å‡ºprobçš„å–binçš„äº¤å‰ç†µï¼Œå°±çŸ¥é“æœ‰æ²¡æœ‰è®¡ç®—å¯¹ä½ç½®
    loss_ce = F.cross_entropy(prob, gt_bin)

    # EMD regularization
    # åœ¨ bin ç»´åº¦ï¼ˆdim=1ï¼‰åšç´¯ç§¯æ±‚å’Œ
    cdf_pred = torch.cumsum(prob, dim=1)
    # æŠŠ(2,296,400)çš„ bin ç¼–å·è½¬æˆç‹¬çƒ­ç¼–ç ï¼Œç»´åº¦å˜æˆ(2,296,400,64)ï¼ˆæ¯”å¦‚ bin=49 çš„ä½ç½®æ˜¯ 1ï¼Œå…¶ä½™æ˜¯ 0ï¼‰
    gt_onehot = F.one_hot(gt_bin, K).permute(0, 3, 1, 2).float()
    # å¯¹çœŸå®ç‹¬çƒ­åˆ†å¸ƒåšç´¯ç§¯æ±‚å’Œï¼Œæ¯”å¦‚çœŸå® bin æ˜¯ 49ï¼Œåˆ™cdf_gtä¸­å‰ 49 ä¸ª bin çš„ç´¯ç§¯å’Œæ˜¯ 0ï¼Œç¬¬ 49 ä¸ªåŠä¹‹åæ˜¯ 1
    cdf_gt = torch.cumsum(gt_onehot, dim=1)
    # è®¡ç®—é¢„æµ‹ç´¯ç§¯åˆ†å¸ƒå’ŒçœŸå®ç´¯ç§¯åˆ†å¸ƒçš„ç»å¯¹å·®ï¼Œå†æ±‚æ‰€æœ‰å…ƒç´ çš„å¹³å‡å€¼ï¼›è¿™ç›¸å½“äºè®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„åœ°çƒç§»åŠ¨è·ç¦»ï¼ˆEMDï¼‰
    loss_emd = torch.mean(torch.abs(cdf_pred - cdf_gt))

    return loss_ce + 0.1 * loss_emd


def depth_bin_loss(prob, gt_depth, depth_bins, gt_mask):
    """
    ä¿®æ­£ç‰ˆï¼šåŠ å…¥gt_maskè¿‡æ»¤æ— æ•ˆåƒç´ ï¼Œä»…è®¡ç®—æœ‰æ•ˆåŒºåŸŸçš„æŸå¤±
    Args:
        prob: (B, K, H, W) - æ¨¡å‹é¢„æµ‹çš„binæ¦‚ç‡
        gt_depth: (B, H, W) - çœŸå®æ·±åº¦å›¾ï¼ˆå«æ— æ•ˆå€¼ï¼Œå¦‚0/NaNï¼‰
        depth_bins: (B, K) or (1, K) - æ·±åº¦binçš„æ•°å€¼
        gt_mask: (B, H, W) - æœ‰æ•ˆåƒç´ æ©ç ï¼Œ1=æœ‰æ•ˆï¼Œ0=æ— æ•ˆï¼ˆfloat/boolç±»å‹å‡å¯ï¼‰
    Returns:
        total_loss: ä»…æœ‰æ•ˆåƒç´ çš„æŸå¤±å€¼
    """
    B, K, H, W = prob.shape
    
    # æ‰©å±•ç»´åº¦ç”¨äºå¹¿æ’­è®¡ç®—
    gt_depth_expanded = gt_depth.unsqueeze(1)  # (B, 1, H, W)
    depth_bins_expanded = depth_bins.view(B, K, 1, 1)  # (B, K, 1, 1)
    
    # -------------------------- 2. è®¡ç®—çœŸå®binï¼ˆå’ŒåŸé€»è¾‘ä¸€è‡´ï¼‰ --------------------------
    dist = torch.abs(gt_depth_expanded - depth_bins_expanded)  # (B, K, H, W)
    gt_bin = torch.argmin(dist, dim=1)  # (B, H, W)
    
    # -------------------------- 3. æ ¸å¿ƒä¿®æ”¹ï¼šåº”ç”¨maskè¿‡æ»¤æ— æ•ˆåƒç´  --------------------------
    # ç»Ÿä¸€maskç±»å‹ï¼šè½¬ä¸ºfloatï¼Œç¡®ä¿ç»´åº¦æ˜¯(B, H, W)ï¼Œ1=æœ‰æ•ˆï¼Œ0=æ— æ•ˆ
    if gt_mask.dtype == torch.bool:
        gt_mask = gt_mask.float()
    # æ‰©å±•maskç»´åº¦ï¼ŒåŒ¹é…probçš„ç»´åº¦ï¼ˆB, 1, H, Wï¼‰ï¼Œæ–¹ä¾¿åç»­å¹¿æ’­
    gt_mask_expanded = gt_mask.unsqueeze(1)  # (B, 1, H, W)
    
    # -------------------------- 4. è®¡ç®—å¸¦maskçš„äº¤å‰ç†µæŸå¤± --------------------------
    # æ–¹æ³•ï¼šå…ˆè®¡ç®—æ‰€æœ‰åƒç´ çš„äº¤å‰ç†µï¼Œå†ç”¨maskåŠ æƒå¹³å‡ï¼ˆä»…æœ‰æ•ˆåƒç´ å‚ä¸ï¼‰
    # æ­¥éª¤1ï¼šè®¡ç®—æ¯ä¸ªåƒç´ çš„äº¤å‰ç†µï¼ˆä¸åšmeanï¼‰
    # F.cross_entropyçš„reduction='none'è¡¨ç¤ºè¿”å›æ¯ä¸ªåƒç´ çš„æŸå¤±å€¼ï¼Œç»´åº¦(B, H, W)
    loss_ce_per_pixel = F.cross_entropy(prob, gt_bin, reduction='none')  # (B, H, W)
    # æ­¥éª¤2ï¼šç”¨maskè¿‡æ»¤æ— æ•ˆåƒç´ ï¼ˆæ— æ•ˆåƒç´ æŸå¤±ç½®0ï¼‰
    loss_ce_masked = loss_ce_per_pixel * gt_mask  # (B, H, W)
    # æ­¥éª¤3ï¼šè®¡ç®—æœ‰æ•ˆåƒç´ çš„å¹³å‡æŸå¤±ï¼ˆé¿å…é™¤ä»¥0ï¼‰
    valid_pixel_num = torch.clamp(gt_mask.sum(), min=1)  # æœ‰æ•ˆåƒç´ æ•°ï¼Œæœ€å°ä¸º1é˜²æ­¢é™¤ä»¥0
    loss_ce = loss_ce_masked.sum() / valid_pixel_num  # æ ‡é‡
    
    # -------------------------- 5. è®¡ç®—å¸¦maskçš„EMDæŸå¤± --------------------------
    # æ­¥éª¤1ï¼šè®¡ç®—ç´¯ç§¯åˆ†å¸ƒï¼ˆå’ŒåŸé€»è¾‘ä¸€è‡´ï¼‰
    cdf_pred = torch.cumsum(prob, dim=1)  # (B, K, H, W)
    gt_onehot = F.one_hot(gt_bin, K).permute(0, 3, 1, 2).float()  # (B, K, H, W)
    cdf_gt = torch.cumsum(gt_onehot, dim=1)  # (B, K, H, W)
    
    # æ­¥éª¤2ï¼šè®¡ç®—æ¯ä¸ªbinã€æ¯ä¸ªåƒç´ çš„EMDç»å¯¹å·®
    emd_per_pixel_per_bin = torch.abs(cdf_pred - cdf_gt)  # (B, K, H, W)
    # æ­¥éª¤3ï¼šç”¨maskè¿‡æ»¤æ— æ•ˆåƒç´ ï¼ˆæ‰©å±•åçš„maskå¹¿æ’­åˆ°Kä¸ªbinç»´åº¦ï¼‰
    emd_masked = emd_per_pixel_per_bin * gt_mask_expanded  # (B, K, H, W)
    # æ­¥éª¤4ï¼šè®¡ç®—æœ‰æ•ˆåƒç´ çš„å¹³å‡EMDæŸå¤±
    loss_emd = emd_masked.sum() / (valid_pixel_num * K)  # æ ‡é‡ï¼ˆé™¤ä»¥æœ‰æ•ˆåƒç´ æ•°Ã—binæ•°ï¼‰
    
    # -------------------------- 6. æ€»æŸå¤± --------------------------
    total_loss = loss_ce + 0.1 * loss_emd
    
    return total_loss
