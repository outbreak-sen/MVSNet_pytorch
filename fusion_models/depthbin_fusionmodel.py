# æŠŠæ·±åº¦å›å½’é—®é¢˜ â†’ æ·±åº¦åˆ†ç±»é—®é¢˜
# å°†æ·±åº¦åˆ’åˆ†ä¸º K ä¸ª bin
# MVS å’Œ DA3 å„è‡ªå¯¹ bin æœ‰ soft prior
# èåˆç½‘ç»œè¾“å‡ºä¸€ä¸ª æ–°çš„ bin åˆ†å¸ƒ
# ç”¨ one-hot / soft-label loss
# ğŸ‘‰ è¿™æ˜¯ CVPR / ICCV æ·±åº¦ä¼°è®¡çš„ä¸»æµæ–¹å‘
# å› ä¸ºDA3çš„confä¸æ˜¯çœŸå®çš„cofï¼ŒDA3 / MVS çš„ conf åªå½±å“ logits

import torch
import torch.nn as nn
import torch.nn.functional as F

class DepthBinFusionNet(nn.Module):
    """
    Depth bin fusion with confidence-aware conditioning
    """
    def __init__(self, num_bins=64, hidden=64):
        super().__init__()
        self.num_bins = num_bins

        self.encoder = nn.Sequential(
            nn.Conv2d(4, hidden, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden, hidden, 3, padding=1),
            nn.ReLU(inplace=True),
        )

        self.bin_logits = nn.Conv2d(hidden, num_bins, 1)

    def forward(self, D_mvs, C_mvs, D_da, C_da):
        """
        conf is treated as raw feature
        """
        x = torch.cat([D_mvs, D_da, C_mvs, C_da], dim=1)
        feat = self.encoder(x)

        logits = self.bin_logits(feat)
        prob = F.softmax(logits, dim=1)

        # depth confidence from entropy
        conf = -torch.sum(prob * torch.log(prob + 1e-8), dim=1, keepdim=True)

        return prob, conf


def depth_bin_loss(prob, gt_depth, depth_bins):
    """
    prob: (B,K,H,W)
    """
    B, K, H, W = prob.shape

    # hard assignment
    gt_bin = torch.argmin(
        torch.abs(gt_depth - depth_bins.view(1, K, 1, 1)),
        dim=1
    )

    loss_ce = F.cross_entropy(prob, gt_bin)

    # EMD regularization
    cdf_pred = torch.cumsum(prob, dim=1)
    gt_onehot = F.one_hot(gt_bin, K).permute(0, 3, 1, 2).float()
    cdf_gt = torch.cumsum(gt_onehot, dim=1)

    loss_emd = torch.mean(torch.abs(cdf_pred - cdf_gt))

    return loss_ce + 0.1 * loss_emd
