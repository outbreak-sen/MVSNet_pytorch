# æŠŠæ·±åº¦å›å½’é—®é¢˜ â†’ æ·±åº¦åˆ†ç±»é—®é¢˜
# å°†æ·±åº¦åˆ’åˆ†ä¸º K ä¸ª bin
# MVS å’Œ DA3 å„è‡ªå¯¹ bin æœ‰ soft prior
# èåˆç½‘ç»œè¾“å‡ºä¸€ä¸ª æ–°çš„ bin åˆ†å¸ƒ
# ç”¨ one-hot / soft-label loss
# ğŸ‘‰ è¿™æ˜¯ CVPR / ICCV æ·±åº¦ä¼°è®¡çš„ä¸»æµæ–¹å‘
# å› ä¸ºDA3çš„confä¸æ˜¯çœŸå®çš„cofï¼ŒDA3 / MVS çš„ conf åªå½±å“ logits

import torch
import torch.nn as nn
import torch.nn.functional as F

class DepthBinFusionNet(nn.Module):
    """
    Depth bin fusion with confidence-aware conditioning
    """
    def __init__(self, num_bins=64, hidden=64):
        super().__init__()
        self.num_bins = num_bins

        self.encoder = nn.Sequential(
            nn.Conv2d(4, hidden, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden, hidden, 3, padding=1),
            nn.ReLU(inplace=True),
        )

        self.bin_logits = nn.Conv2d(hidden, num_bins, 1)

    def forward(self, D_mvs, C_mvs, D_da, C_da):
        """
        conf is treated as raw feature
        """
        x = torch.cat([D_mvs, D_da, C_mvs, C_da], dim=1)
        feat = self.encoder(x)

        logits = self.bin_logits(feat)
        prob = F.softmax(logits, dim=1)

        # depth confidence from entropy
        conf = -torch.sum(prob * torch.log(prob + 1e-8), dim=1, keepdim=True)

        return prob, conf


def depth_bin_loss(prob, gt_depth, depth_bins):
    """
    prob: (B, K, H, W)
    gt_depth: (B, H, W)
    depth_bins: (B, K) or (1, K)
    """
    B, K, H, W = prob.shape
    
    # ç¡®ä¿ depth_bins æœ‰æ­£ç¡®çš„å½¢çŠ¶
    if depth_bins.dim() == 2:
        # depth_bins: (B, K) or (1, K)
        # å¦‚æœæ˜¯ (1, K) ä¸” B > 1ï¼Œåˆ™å¹¿æ’­åˆ°æ•´ä¸ªbatch
        if depth_bins.shape[0] == 1 and B > 1:
            depth_bins = depth_bins.expand(B, -1)
    
    # ä¿®æ­£ç»´åº¦åŒ¹é…é—®é¢˜
    # gt_depth: (B, H, W) -> (B, 1, H, W)
    # depth_bins: (B, K) -> (B, K, 1, 1)
    # è¿™æ ·å¹¿æ’­åå¾—åˆ°: (B, K, H, W)
    gt_depth_expanded = gt_depth.unsqueeze(1)  # (B, 1, H, W)
    depth_bins_expanded = depth_bins.view(B, K, 1, 1)  # (B, K, 1, 1)
    
    # hard assignment
    # è®¡ç®—æ¯ä¸ªåƒç´ ç‚¹ä¸æ¯ä¸ªæ·±åº¦åŒºé—´çš„è·ç¦»
    dist = torch.abs(gt_depth_expanded - depth_bins_expanded)  # (B, K, H, W)
    gt_bin = torch.argmin(dist, dim=1)  # (B, H, W)
    
    loss_ce = F.cross_entropy(prob, gt_bin)

    # EMD regularization
    cdf_pred = torch.cumsum(prob, dim=1)
    gt_onehot = F.one_hot(gt_bin, K).permute(0, 3, 1, 2).float()
    cdf_gt = torch.cumsum(gt_onehot, dim=1)

    loss_emd = torch.mean(torch.abs(cdf_pred - cdf_gt))

    return loss_ce + 0.1 * loss_emd